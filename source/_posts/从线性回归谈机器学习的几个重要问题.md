---
title: 从线性回归谈机器学习的几个重要问题
date: 2016-07-31 11:25:54
tags: 
- 机器学习
- 线性回归
- 损失函数
- 梯度下降法
- 学习率
- 特征缩放

categories:
- 机器学习
---

### 线性回归问题描述

对于这个问题，举个实际例子更好理解：你想购买一套房子，现知道这套房子的面积和一些数据，这些数据包括一些房子的面积大小和售价，假如房子的面积 $x$ 和售价 $y$ 存在线性关系 $y = ax + b$，那么根据历史数据求出这个线性关系中的 $a$ 和 $b$ 就是一个最简单一维的线性回归问题。求出这个 $y = ax + b$后，代入当前房子的面积即可估算出当前房子的售价。

<!-- more -->

当然，影响房子价格的因素还有很多，如：房子的地段，盖了多少年等等。如果这些因素有 $n$ 个，并假设这 $n$ 个因素与房子价格之间是线性关系，那么这个问题即是一个多维线性回归问题，其线性关系表示为：
$$
    y = \theta ^\mathrm{T}x \tag{1}
$$ 其中 $\theta = (\theta_0, \theta_1, \theta_2, \cdots, \theta_n)^T$，$x = (x_0, x_1, x_2, \cdots, x_n)^T$，通常 $ x_0 = 1 $。当 $ n = 1 $时，即是一个一维线性回归问题。

### 机器学习

对于上面的线性回归问题，程序需要从历史数据中学习一个线性关系 $ y = \theta ^\mathrm{T} x $，并使得这个关系尽可能接近数据所隐含的关系。这样的过程称为机器学习。

关于机器学习的定义有很多，比较认同的是Andrew Ng在其机器学习课程中所介绍的: 
> A computer program is said to learn from experience E with respect to some task T and some performance measure P if its performance on T as measured by P and improved with experience E

上面问题中历史数据就是E，要学习的线性关系就是T（通常会假设T属于某类关系 $ h(x) $，称之为假设关系），衡量学得的关系与数据中隐含关系差距的方法就是P。

曾经看到过这样一种观点，认为价值观就是n维空间到1维空间的映射。因为现实中的很多东西的属性天然就是高维的，需要向量来表示，如上面所说的房子。但是向量是无法比较大小的，人们都是用从东西的属性中提取出的标量（如房子的价格）来进行比较，提取的方式就是价值观。不同的人提取出来的标量不尽相同，这就是价值观的差别。

机器学习方面的问题，无非分为两类：分类和回归。解决问题的方法都是将高维向量（特征），通过模型映射为标量，然后进行一种比较。这与价值观的工作方式何其相似，是否可认为价值观就是一种模型？或者模型就是价值观的另一种叫法？如果上述观点成立，那么，机器学习的过程是否就等同于价值观的塑造过程？！这样理解的话，Siri之所以可以被教坏的原因就不难理解了。扯远了~

### 损失函数

使用Andrew介绍的定义，损失函数就是P，确切的说就是衡量假设关系 $ h(x) $ 与真实关系 $ y $之间的差距。最常用的损失函数是平方损失函数，具体到线性回归模型中就是：
{% raw %}
$$
    J(\theta) = \frac{1}{2m} \sum_ {i = 1}^{m} h_\theta(x^{(i)}) - y^{(i)} \tag{2}
$$ 其中 $ J(\theta)$ 表示损失函数，$ h_ \theta(x) = \theta ^ \mathrm{T} x$，为方便标记，后面将 $ h_\theta(x) $记为 $ h(x) $。
{% endraw %}
上面这个公式的意思就是求每个样本通过 $ h(x) $ 求得的值与实际值差的平均值。机器学习过程就是：
{% raw %}
$$
    \min_\theta J(\theta) \tag{3}
$$
{% endraw %}

### 梯度下降法

可以使用迭代的方法求解式(3)，其中一种常用方法就是梯度下降法。梯度是高数里面的一个概念，其意义是：沿着梯度方向，函数下降最快。变量 $x$ 的梯度等于函数对 $x$ 求偏导，即 $ \frac{\delta f(x)}{\delta x} $。
使用梯度下降法求解式(3)的具体过程如下：
{% raw %}
$
    \text{Repeat until convergence}\lbrace\\
    \qquad \theta_ j = \theta_ j - \alpha \frac{\delta J(\theta)}{\delta \theta_ j} 
    \qquad(j = 0,1,2,\cdots,n) \\
    \rbrace\\
$
<br>
其中，
$ 
    \frac{\delta J(\theta)}{\delta \theta_ j} = 
    \frac{1}{m} \sum_ {i = 1}^m[h_ (x^{(i)} - y^{(i)})]x_j^{(i)} 
$，$ \alpha $ 称之为学习率
{% endraw %}

### 学习率

学习率，顾名思义，是对学习速率的一种衡量，如果 $ \alpha $ 偏小，收敛速度会慢，如果 $ \alpha $ 偏大，这有可能导致不收敛，甚至会导致 $ J(\theta) $ 越来越大。实际问题中，$ \alpha $是调节的参数。

一个误区是：认为迭代过程中随着越接近局部或全局的最小值，$ \alpha $ 需要自动变小。因为随着越接近收敛值，梯度$ \frac{\delta J(\theta)}{\theta_j} $ 越来越小，$ \alpha $ 没有必要变动。

### 特征缩放

实际中，训练模型之前都需要对特征进行缩放，使其落在同一个区间中。**目的是让迭代过程更快**。

将买房子的例子来说，只考虑房子的面积 $ x\_ 1 $ 和房间个数 $ x\_2 $ 两个因素。如果 $ x\_1 \in [0, 1000] $，$ x\_2 \in [1, 10] $, 因为 $ x\_1, x\_2 $ 大小差别太大，$ J(\theta) $ 的曲面必定狭长，在 $ x\_1$ 轴方向上很长， 在 $ x\_2 $ 轴方向很窄。在这种情况下，如果初始参数选择不当，会造成迭代缓慢。

常用的特征缩放方式有两种：
1. 离差标准化
$$
x^{\prime} = \frac{x - min}{max - min} \tag{4} 
$$
2. 均值正则化
$$ 
x^{\prime} = \frac{x - \mu}{\sigma} \tag{5}
$$

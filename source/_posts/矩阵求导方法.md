---
title: 矩阵求导方法
date: 2017-03-04 21:24:46
tags:
- 数学
- 矩阵
categories:
- 机器学习
---

在机器学习的公式推导中大量运用了矩阵求导的知识，以前只是凭着函数求导的感觉来做，碰到矩阵转置的时候总是弄不清楚到底要不要转置。这几天看了下矩阵求导方面的知识，总结如下。

在本文中向量默认用列向量表示。

<!--more-->

## Layout Conventions

这是在wiki上看到的词，不太好翻译，我理解的意思是矩阵元素的摆放方式。向量$\mathbf y \in\mathbb R^m$对向量$\mathbf x \in \mathbb R^n$求导后数据有两种摆放形式：

1. Numerator layout: 求导后矩阵维数由$\mathbf y$和$\mathbf x^{\mathrm T}$决定，是$m \times n$ ，最后一维与$\mathbf x^{\mathrm T}$相同。
2. Denominator layout: 求导后矩阵维数是$n \times m$，由$\mathbf y^{\mathrm T}$和$\mathbf x$决定，最后一维与$\mathbf y^{\mathrm T}$相同

用$\frac {\partial y}{\partial \mathbf x}$举例（变量可以看作是长度唯一的向量），使用Numerator layout得到的是一个列向量，而使用Denominator layout得到的是一个行向量。

这两种方式得到的结果是一样的，只不过一个是另一个的转置。在本文中默认使用Numerator layout方式。

## 元素级求导

矩阵$\mathbf A$对矩阵$\mathbf B$求导（向量和标量可以看做是特殊矩阵），实际上是$\mathbf A$中每个元素中$\mathbf B$中每个元素求导，然后把所有导数按照一定的摆放方式排成一个矩阵或张量。

### 向量对标量

对于向量$\mathbf y = \left[  \begin{array}{c}  y_1\\\ y_2  \\\ \vdots \\\ y_m \end{array} \right]$， 其对标量$x$求导的结果是：$\frac{\partial \mathbf y}{\partial x} = \begin{bmatrix}\frac{\partial y_1}{\partial x} \\\ \frac{\partial y_2}{\partial x}\\\ \vdots \\\ \frac{\partial y_m}{\partial x}\end{bmatrix}$



### 标量对向量

对于标量$y$，其对向量$\mathbf x = \begin{bmatrix} x_1 \\\ x_2 \\\ \vdots \\\ x_n \end{bmatrix}$求导的结果是：$\frac{\partial y}{\partial \mathbf x} =\begin{bmatrix} \frac{\partial y}{\partial x_1} &\frac{\partial y}{\partial x_2} &\cdots &\frac{\partial y}{\partial x_n}\end{bmatrix}$



### 向量对向量

向量$\mathbf y = \left[  \begin{array}{c}  y_1\\\ y_2  \\\ \vdots \\\ y_m \end{array} \right]$ 对向量$\mathbf x = \begin{bmatrix} x_1 \\\ x_2 \\\ \vdots \\\ x_n \end{bmatrix}$求导的结果是：$\frac{\partial \mathbf y}{\partial \mathbf x} = \begin{bmatrix}\frac{\partial y_1}{\partial x_1} &  \frac{\partial y_1}{\partial x_2}  & \cdots  &\frac{\partial y_1}{\partial x_n} \\\ \frac{\partial y_2}{\partial x_1} &  \frac{\partial y_2}{\partial x_2}  & \cdots  &\frac{\partial y_2}{\partial x_n} \\\ \vdots &  \vdots &  \ddots & \vdots \\\ \frac{\partial y_m}{\partial x_1} &  \frac{\partial y_m}{\partial x_2}  & \cdots  &\frac{\partial y_m}{\partial x_n} \end{bmatrix}$



### 其他

矩阵$\mathbf Y$对$x$求导时，$\mathbf Y$中的每个元素分别对$x$求导，$\frac{\partial \mathbf Y}{\partial x}$的形状与$\mathbf Y$相同。$\frac{\partial x}{\partial \mathbf Y}$的形状与$\mathbf Y^{\mathrm T}$相同。

矩阵与向量之间的求导在平面无法展开，大体思想相同：分子对分母里面的每个元素按照上面规则求导，最后将结果写成一个张量。

## 公式求导

函数求导中的乘法法则，加法法则和链式法则也可适用到矩阵公式求导中。

对矩阵公式求导时，先弄清楚分子分母最后的结果分别是什么（如分子是标量，分母是向量），然后根据元素级求导法则确定最后所求的的形状应该什么样的。根据最后的形状在求导过程中处理转置问题。

### 普通公式求导

下面通过几个例子讲解公式求导方法

假设$\mathbf x \in \mathbb R^{n}$，$\mathbf A \in \mathbb R ^ {n \times m}$

1. $\frac{\partial \mathbf x^{\mathrm T} \mathbf A}{\partial \mathbf x} = \mathbf A^{\mathrm T}$

   分子分母都是一个向量，最后的结果是一个矩阵，该矩阵的维数是$m \times n​$(看Layout Conventions部分)，所以最后的结果是$\mathbf A ^{\mathrm T}$而不是$\mathbf A$。

2. $\frac{\partial \mathbf x^{\mathrm T} \mathbf A \mathbf x}{\partial \mathbf x} = \mathbf x ^ {\mathrm T}(\mathbf A + \mathbf A^{\mathrm T})$

   分子是一个数，分母是一个向量，故最后的结果也是一个向量，与$\mathbf x ^ {\mathrm T}$形状相同，故其推导过程如下：
   $$
   \frac{\partial \mathbf x^{\mathrm T} \mathbf A \mathbf x}{\partial \mathbf x} 
   =\mathbf x^{\mathrm T} \frac{\partial \mathbf x ^ {\mathrm T} \mathbf A}{\partial \mathbf x} + \mathbf x ^ {\mathrm T} \mathbf A 
   = \mathbf{x^{\mathrm T}A^{\mathrm T}+x^{\mathrm T}A}
   =\mathbf{x^{\mathrm T}(A+A^{\mathrm T})}
   $$

3. $\mathbf{\frac{\partial (Ax+b) ^ {\mathrm T}C(Dx+e)}{\partial x} = (Dx+e)^{\mathrm T}C^{\mathrm T}A+(Ax+b)^{\mathrm T}CD}$

   分母是个标量，分子是个向量，最后结果也是个向量，与$\mathbf x ^ {\mathrm T}$形状相同。
   $$
   \begin{align}
   \frac{\mathbf{\partial (Ax+b) ^ {\mathrm T}C(Dx+e)}}{\partial \mathbf x}
   &=\mathbf{ [C(Dx+e)]^{\mathrm T}\frac{\partial (Ax+b) ^ {\mathrm T}}{\partial x} + (Ax+b)^{\mathrm T}C\frac{\partial (Dx+e)}{\partial x}}\\\
   &= \mathbf{(Dx+e)^{\mathrm T}C^{\mathrm T} \frac{\partial x^{\mathrm T}A^{\mathrm T}}{x}+(Ax+b)^{\mathrm T}CD}\\\
   &=\mathbf{(Dx+e)^{\mathrm T}C^{\mathrm T}A +(Ax+b)^{\mathrm T}CD}
   \end{align}
   $$



### 矩阵的迹求导

充分利用矩阵迹的性质：

1. $tr(A) = tr(A^{\mathrm T})$
2. $tr(ABCD)=tr(BCDA)=tr(CDAB)=tr(DABC)$
3. $tr(m\mathbf A + n \mathbf B)=mtr(\mathbf A)+n tr(\mathbf B)$
4. 如果$d\mathbf y = tr(\mathbf Ad \mathbf X)$，则$\frac{d\mathbf y}{d \mathbf X}=\mathbf A $

例如计算：$\frac{\partial tr(\mathbf {AXBX^{\mathrm T}C})}{\partial \mathbf X}$
$$
\begin{align}
 d \mathrm tr (\mathbf{AXBX^{\mathrm T}C})
 &= d \mathrm tr(\mathbf {CAXBX^{\mathrm T}})=\mathrm tr(d(\mathbf{CAX})\mathbf {BX^{\mathrm T}}+\mathbf{CAX}d(\mathbf{BX^{\mathrm T}}))\\\
 &= \mathrm tr(d(\mathbf{CAX})\mathbf {BX^{\mathrm T}})+\mathrm tr(\mathbf{CAX}d(\mathbf{BX^{\mathrm T}}))\\\
 &=\mathrm tr(\mathbf{CA}d(\mathbf X)\mathbf{BX^{\mathrm T}})+\mathrm tr(\mathbf{CAXB}d(\mathbf{X^\mathrm T}))\\\
 &=\mathrm tr(\mathbf{BX^{\mathrm T}CA}d(\mathbf X)) + \mathrm tr(d(\mathbf X)\mathbf{B^{\mathrm T}X^{\mathrm T}A^{\mathrm T}C^{\mathrm T}})\\\
 &=\mathrm tr(\mathbf{BX^{\mathrm T}CA}d(\mathbf X)) +\mathrm tr(\mathbf{B^{\mathrm T}X^{\mathrm T}A^{\mathrm T}C^{\mathrm T}}d(\mathbf X))\\\
 &=\mathrm tr((\mathbf{BX^{\mathrm T}CA} +\mathbf{B^{\mathrm T}X^{\mathrm T}A^{\mathrm T}C^{\mathrm T}})d(\mathbf X))\\\
\end{align}
$$
所以：
$$
\frac{\partial tr(\mathbf {AXBX^{\mathrm T}C})}{\partial \mathbf X}
=\mathbf{BX^{\mathrm T}CA} +\mathbf{B^{\mathrm T}X^{\mathrm T}A^{\mathrm T}C^{\mathrm T}}
$$


## 总结

知道以上方法基本上机器学习里面关于矩阵求导的问题都可以解决了，如果遇到更复杂的请参考[wiki](https://en.wikipedia.org/wiki/Matrix_calculus)